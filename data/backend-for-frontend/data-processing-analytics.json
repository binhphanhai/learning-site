{
  "title": "Data Processing & Analytics",
  "description": "Learn data processing pipelines, analytics integration, search functionality, and file handling for building data-driven applications that power modern frontend experiences.",
  "sections": [
    {
      "id": "data-pipelines",
      "title": "Data Pipelines & ETL",
      "content": [
        {
          "type": "heading",
          "text": "ETL (Extract, Transform, Load) Processes"
        },
        {
          "type": "paragraph",
          "text": "ETL processes are essential for moving and transforming data between systems, enabling analytics and reporting capabilities."
        },
        {
          "type": "code",
          "language": "javascript",
          "text": "// ETL Pipeline Implementation\nclass ETLPipeline {\n  constructor() {\n    this.extractors = new Map();\n    this.transformers = [];\n    this.loaders = new Map();\n  }\n  \n  // Extract data from various sources\n  addExtractor(name, extractor) {\n    this.extractors.set(name, extractor);\n  }\n  \n  addTransformer(transformer) {\n    this.transformers.push(transformer);\n  }\n  \n  addLoader(name, loader) {\n    this.loaders.set(name, loader);\n  }\n  \n  async run(sourceName, targetName) {\n    try {\n      console.log(`Starting ETL pipeline: ${sourceName} -> ${targetName}`);\n      \n      // Extract\n      const extractor = this.extractors.get(sourceName);\n      if (!extractor) throw new Error(`Extractor '${sourceName}' not found`);\n      \n      const rawData = await extractor.extract();\n      console.log(`Extracted ${rawData.length} records`);\n      \n      // Transform\n      let transformedData = rawData;\n      for (const transformer of this.transformers) {\n        transformedData = await transformer.transform(transformedData);\n        console.log(`Transformed to ${transformedData.length} records`);\n      }\n      \n      // Load\n      const loader = this.loaders.get(targetName);\n      if (!loader) throw new Error(`Loader '${targetName}' not found`);\n      \n      await loader.load(transformedData);\n      console.log(`Loaded ${transformedData.length} records to ${targetName}`);\n      \n      return { success: true, recordsProcessed: transformedData.length };\n    } catch (error) {\n      console.error('ETL Pipeline failed:', error);\n      throw error;\n    }\n  }\n}\n\n// Database extractor\nclass DatabaseExtractor {\n  constructor(connectionString, query) {\n    this.connectionString = connectionString;\n    this.query = query;\n  }\n  \n  async extract() {\n    const db = await connectToDatabase(this.connectionString);\n    const results = await db.query(this.query);\n    await db.close();\n    return results;\n  }\n}\n\n// API extractor\nclass APIExtractor {\n  constructor(apiUrl, headers = {}) {\n    this.apiUrl = apiUrl;\n    this.headers = headers;\n  }\n  \n  async extract() {\n    const response = await fetch(this.apiUrl, {\n      headers: this.headers\n    });\n    \n    if (!response.ok) {\n      throw new Error(`API extraction failed: ${response.statusText}`);\n    }\n    \n    const data = await response.json();\n    return Array.isArray(data) ? data : [data];\n  }\n}\n\n// Data transformer\nclass DataTransformer {\n  constructor(transformFn) {\n    this.transformFn = transformFn;\n  }\n  \n  async transform(data) {\n    return data.map(this.transformFn).filter(item => item !== null);\n  }\n}\n\n// Usage example\nconst pipeline = new ETLPipeline();\n\n// Set up extractors\npipeline.addExtractor('orders', new DatabaseExtractor(\n  'postgresql://localhost/orders',\n  'SELECT * FROM orders WHERE created_at >= CURRENT_DATE - INTERVAL \\'1 day\\''\n));\n\npipeline.addExtractor('users', new APIExtractor(\n  'https://api.example.com/users',\n  { 'Authorization': 'Bearer token123' }\n));\n\n// Set up transformers\npipeline.addTransformer(new DataTransformer(order => {\n  if (!order.total_amount || order.total_amount <= 0) return null;\n  \n  return {\n    orderId: order.id,\n    userId: order.user_id,\n    totalAmount: parseFloat(order.total_amount),\n    orderDate: new Date(order.created_at).toISOString(),\n    status: order.status.toLowerCase()\n  };\n}));\n\n// Set up loaders\npipeline.addLoader('analytics', new DatabaseLoader(\n  'postgresql://localhost/analytics',\n  'daily_orders'\n));\n\n// Run pipeline\npipeline.run('orders', 'analytics')\n  .then(result => console.log('Pipeline completed:', result))\n  .catch(error => console.error('Pipeline failed:', error));"
        },
        {
          "type": "heading",
          "text": "Stream Processing"
        },
        {
          "type": "paragraph",
          "text": "Real-time data processing for handling continuous data streams."
        },
        {
          "type": "code",
          "language": "javascript",
          "text": "// Stream processing with Node.js streams\nconst { Transform, pipeline } = require('stream');\nconst fs = require('fs');\n\nclass DataProcessor extends Transform {\n  constructor(options = {}) {\n    super({ objectMode: true, ...options });\n    this.batchSize = options.batchSize || 100;\n    this.batch = [];\n  }\n  \n  _transform(chunk, encoding, callback) {\n    try {\n      const data = JSON.parse(chunk.toString());\n      \n      // Process individual record\n      const processed = this.processRecord(data);\n      \n      if (processed) {\n        this.batch.push(processed);\n        \n        if (this.batch.length >= this.batchSize) {\n          this.push(JSON.stringify(this.batch) + '\\n');\n          this.batch = [];\n        }\n      }\n      \n      callback();\n    } catch (error) {\n      callback(error);\n    }\n  }\n  \n  _flush(callback) {\n    if (this.batch.length > 0) {\n      this.push(JSON.stringify(this.batch) + '\\n');\n    }\n    callback();\n  }\n  \n  processRecord(data) {\n    // Custom processing logic\n    if (!data.timestamp || !data.event) return null;\n    \n    return {\n      timestamp: new Date(data.timestamp).toISOString(),\n      event: data.event,\n      userId: data.userId,\n      properties: data.properties || {},\n      processed_at: new Date().toISOString()\n    };\n  }\n}\n\n// Stream processing pipeline\nconst processEventStream = () => {\n  const inputStream = fs.createReadStream('events.jsonl');\n  const processor = new DataProcessor({ batchSize: 50 });\n  const outputStream = fs.createWriteStream('processed_events.jsonl');\n  \n  pipeline(\n    inputStream,\n    processor,\n    outputStream,\n    (error) => {\n      if (error) {\n        console.error('Stream processing failed:', error);\n      } else {\n        console.log('Stream processing completed successfully');\n      }\n    }\n  );\n};\n\n// Real-time stream processing with Redis Streams\nclass RedisStreamProcessor {\n  constructor(redisClient, streamName) {\n    this.redis = redisClient;\n    this.streamName = streamName;\n    this.running = false;\n  }\n  \n  async start() {\n    this.running = true;\n    let lastId = '$'; // Start from newest messages\n    \n    while (this.running) {\n      try {\n        const results = await this.redis.xread(\n          'BLOCK', 1000,\n          'STREAMS', this.streamName, lastId\n        );\n        \n        if (results) {\n          for (const [stream, messages] of results) {\n            for (const [id, fields] of messages) {\n              await this.processMessage(id, fields);\n              lastId = id;\n            }\n          }\n        }\n      } catch (error) {\n        console.error('Stream processing error:', error);\n        await new Promise(resolve => setTimeout(resolve, 5000));\n      }\n    }\n  }\n  \n  async processMessage(id, fields) {\n    const data = {};\n    for (let i = 0; i < fields.length; i += 2) {\n      data[fields[i]] = fields[i + 1];\n    }\n    \n    console.log(`Processing message ${id}:`, data);\n    \n    // Process the message\n    const processed = {\n      originalId: id,\n      ...data,\n      processedAt: Date.now()\n    };\n    \n    // Store processed data\n    await this.storeProcessedData(processed);\n  }\n  \n  async storeProcessedData(data) {\n    // Store in database or send to another system\n    await this.redis.lpush('processed_events', JSON.stringify(data));\n  }\n  \n  stop() {\n    this.running = false;\n  }\n}"
        }
      ]
    },
    {
      "id": "analytics-integration",
      "title": "Analytics Integration",
      "content": [
        {
          "type": "heading",
          "text": "Event Tracking System"
        },
        {
          "type": "paragraph",
          "text": "Implement comprehensive event tracking for user behavior analysis and business intelligence."
        },
        {
          "type": "code",
          "language": "javascript",
          "text": "// Analytics event tracking system\nclass AnalyticsTracker {\n  constructor(options = {}) {\n    this.apiUrl = options.apiUrl || '/api/analytics/events';\n    this.batchSize = options.batchSize || 10;\n    this.flushInterval = options.flushInterval || 30000; // 30 seconds\n    this.eventQueue = [];\n    this.startBatchProcessor();\n  }\n  \n  // Track individual events\n  track(eventName, properties = {}, userId = null) {\n    const event = {\n      id: this.generateEventId(),\n      name: eventName,\n      properties: {\n        ...properties,\n        timestamp: new Date().toISOString(),\n        url: typeof window !== 'undefined' ? window.location.href : null,\n        userAgent: typeof navigator !== 'undefined' ? navigator.userAgent : null\n      },\n      userId,\n      sessionId: this.getSessionId()\n    };\n    \n    this.eventQueue.push(event);\n    \n    if (this.eventQueue.length >= this.batchSize) {\n      this.flush();\n    }\n    \n    return event.id;\n  }\n  \n  // Track page views\n  trackPageView(pageName, properties = {}) {\n    return this.track('page_view', {\n      page: pageName,\n      ...properties\n    });\n  }\n  \n  // Track user interactions\n  trackClick(element, properties = {}) {\n    return this.track('click', {\n      element: element.tagName?.toLowerCase(),\n      id: element.id,\n      className: element.className,\n      text: element.textContent?.slice(0, 100),\n      ...properties\n    });\n  }\n  \n  // Track form submissions\n  trackFormSubmission(formName, formData, properties = {}) {\n    return this.track('form_submit', {\n      form: formName,\n      fields: Object.keys(formData),\n      ...properties\n    });\n  }\n  \n  // Track ecommerce events\n  trackPurchase(orderId, items, totalAmount, properties = {}) {\n    return this.track('purchase', {\n      orderId,\n      items: items.map(item => ({\n        productId: item.id,\n        name: item.name,\n        category: item.category,\n        price: item.price,\n        quantity: item.quantity\n      })),\n      totalAmount,\n      itemCount: items.length,\n      ...properties\n    });\n  }\n  \n  // Flush events to server\n  async flush() {\n    if (this.eventQueue.length === 0) return;\n    \n    const eventsToSend = [...this.eventQueue];\n    this.eventQueue = [];\n    \n    try {\n      const response = await fetch(this.apiUrl, {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json'\n        },\n        body: JSON.stringify({ events: eventsToSend })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Analytics API error: ${response.statusText}`);\n      }\n      \n      console.log(`Sent ${eventsToSend.length} analytics events`);\n    } catch (error) {\n      console.error('Failed to send analytics events:', error);\n      // Re-queue events for retry\n      this.eventQueue.unshift(...eventsToSend);\n    }\n  }\n  \n  startBatchProcessor() {\n    setInterval(() => {\n      this.flush();\n    }, this.flushInterval);\n    \n    // Flush on page unload\n    if (typeof window !== 'undefined') {\n      window.addEventListener('beforeunload', () => {\n        // Use sendBeacon for reliable delivery\n        if (this.eventQueue.length > 0 && navigator.sendBeacon) {\n          navigator.sendBeacon(\n            this.apiUrl,\n            JSON.stringify({ events: this.eventQueue })\n          );\n        }\n      });\n    }\n  }\n  \n  generateEventId() {\n    return `${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;\n  }\n  \n  getSessionId() {\n    if (typeof window === 'undefined') return null;\n    \n    let sessionId = sessionStorage.getItem('analytics_session_id');\n    if (!sessionId) {\n      sessionId = this.generateEventId();\n      sessionStorage.setItem('analytics_session_id', sessionId);\n    }\n    return sessionId;\n  }\n}\n\n// Backend analytics API\nconst express = require('express');\nconst app = express();\n\nclass AnalyticsService {\n  constructor() {\n    this.eventProcessors = new Map();\n    this.setupDefaultProcessors();\n  }\n  \n  setupDefaultProcessors() {\n    // Page view processor\n    this.eventProcessors.set('page_view', async (event) => {\n      await this.storeEvent('page_views', {\n        ...event,\n        page: event.properties.page,\n        url: event.properties.url,\n        timestamp: event.properties.timestamp\n      });\n    });\n    \n    // Purchase processor\n    this.eventProcessors.set('purchase', async (event) => {\n      await this.storeEvent('purchases', event);\n      \n      // Update revenue metrics\n      await this.updateRevenueMetrics(event);\n      \n      // Update product analytics\n      for (const item of event.properties.items) {\n        await this.updateProductMetrics(item);\n      }\n    });\n  }\n  \n  async processEvents(events) {\n    const results = [];\n    \n    for (const event of events) {\n      try {\n        const processor = this.eventProcessors.get(event.name);\n        if (processor) {\n          await processor(event);\n        } else {\n          await this.storeEvent('raw_events', event);\n        }\n        \n        results.push({ eventId: event.id, status: 'processed' });\n      } catch (error) {\n        console.error(`Failed to process event ${event.id}:`, error);\n        results.push({ eventId: event.id, status: 'failed', error: error.message });\n      }\n    }\n    \n    return results;\n  }\n  \n  async storeEvent(table, event) {\n    // Store in database (implementation depends on your database)\n    await db.collection(table).insertOne({\n      ...event,\n      stored_at: new Date()\n    });\n  }\n  \n  async updateRevenueMetrics(event) {\n    const date = new Date(event.properties.timestamp).toISOString().split('T')[0];\n    \n    await db.collection('daily_revenue').updateOne(\n      { date },\n      {\n        $inc: {\n          total_revenue: event.properties.totalAmount,\n          order_count: 1,\n          item_count: event.properties.itemCount\n        }\n      },\n      { upsert: true }\n    );\n  }\n  \n  async updateProductMetrics(item) {\n    await db.collection('product_metrics').updateOne(\n      { product_id: item.productId },\n      {\n        $inc: {\n          sales_count: item.quantity,\n          revenue: item.price * item.quantity\n        },\n        $set: {\n          last_sold: new Date()\n        }\n      },\n      { upsert: true }\n    );\n  }\n}\n\n// Analytics API endpoint\nconst analyticsService = new AnalyticsService();\n\napp.post('/api/analytics/events', async (req, res) => {\n  try {\n    const { events } = req.body;\n    \n    if (!Array.isArray(events)) {\n      return res.status(400).json({ error: 'Events must be an array' });\n    }\n    \n    const results = await analyticsService.processEvents(events);\n    \n    res.json({\n      success: true,\n      processed: results.length,\n      results\n    });\n  } catch (error) {\n    console.error('Analytics API error:', error);\n    res.status(500).json({ error: 'Internal server error' });\n  }\n});"
        }
      ]
    },
    {
      "id": "search-functionality",
      "title": "Search Functionality",
      "content": [
        {
          "type": "heading",
          "text": "Full-Text Search with Elasticsearch"
        },
        {
          "type": "paragraph",
          "text": "Implement powerful search capabilities using Elasticsearch for complex queries and relevance scoring."
        },
        {
          "type": "code",
          "language": "javascript",
          "text": "// Elasticsearch client setup and search implementation\nconst { Client } = require('@elastic/elasticsearch');\n\nclass SearchService {\n  constructor() {\n    this.client = new Client({\n      node: process.env.ELASTICSEARCH_URL || 'http://localhost:9200'\n    });\n  }\n  \n  // Index document\n  async indexDocument(index, id, document) {\n    try {\n      const response = await this.client.index({\n        index,\n        id,\n        body: document\n      });\n      \n      return response.body;\n    } catch (error) {\n      console.error('Indexing error:', error);\n      throw error;\n    }\n  }\n  \n  // Bulk index documents\n  async bulkIndex(index, documents) {\n    const body = documents.flatMap(doc => [\n      { index: { _index: index, _id: doc.id } },\n      doc\n    ]);\n    \n    try {\n      const response = await this.client.bulk({ body });\n      \n      if (response.body.errors) {\n        const errors = response.body.items.filter(item => item.index.error);\n        console.error('Bulk indexing errors:', errors);\n      }\n      \n      return response.body;\n    } catch (error) {\n      console.error('Bulk indexing error:', error);\n      throw error;\n    }\n  }\n  \n  // Basic search\n  async search(index, query, options = {}) {\n    const searchParams = {\n      index,\n      body: {\n        query,\n        from: options.from || 0,\n        size: options.size || 10,\n        sort: options.sort || [],\n        highlight: options.highlight || {}\n      }\n    };\n    \n    if (options.aggregations) {\n      searchParams.body.aggs = options.aggregations;\n    }\n    \n    try {\n      const response = await this.client.search(searchParams);\n      \n      return {\n        hits: response.body.hits.hits.map(hit => ({\n          id: hit._id,\n          score: hit._score,\n          source: hit._source,\n          highlight: hit.highlight\n        })),\n        total: response.body.hits.total.value,\n        aggregations: response.body.aggregations\n      };\n    } catch (error) {\n      console.error('Search error:', error);\n      throw error;\n    }\n  }\n  \n  // Multi-field search with boosting\n  async multiFieldSearch(index, searchTerm, options = {}) {\n    const query = {\n      bool: {\n        should: [\n          {\n            multi_match: {\n              query: searchTerm,\n              fields: [\n                'title^3',    // Boost title matches\n                'description^2',\n                'tags',\n                'content'\n              ],\n              type: 'best_fields',\n              fuzziness: 'AUTO'\n            }\n          },\n          {\n            match_phrase: {\n              title: {\n                query: searchTerm,\n                boost: 5    // Boost exact phrase matches\n              }\n            }\n          }\n        ],\n        minimum_should_match: 1\n      }\n    };\n    \n    // Add filters\n    if (options.filters) {\n      query.bool.filter = options.filters;\n    }\n    \n    return this.search(index, query, options);\n  }\n  \n  // Faceted search\n  async facetedSearch(index, searchTerm, facets = {}) {\n    const query = searchTerm ? {\n      multi_match: {\n        query: searchTerm,\n        fields: ['title', 'description', 'content']\n      }\n    } : { match_all: {} };\n    \n    const aggregations = {};\n    \n    // Build aggregations for facets\n    Object.entries(facets).forEach(([facetName, facetConfig]) => {\n      if (facetConfig.type === 'terms') {\n        aggregations[facetName] = {\n          terms: {\n            field: facetConfig.field,\n            size: facetConfig.size || 10\n          }\n        };\n      } else if (facetConfig.type === 'range') {\n        aggregations[facetName] = {\n          range: {\n            field: facetConfig.field,\n            ranges: facetConfig.ranges\n          }\n        };\n      }\n    });\n    \n    const result = await this.search(index, query, {\n      aggregations,\n      size: 20\n    });\n    \n    // Format facets\n    const formattedFacets = {};\n    if (result.aggregations) {\n      Object.entries(result.aggregations).forEach(([facetName, aggResult]) => {\n        if (aggResult.buckets) {\n          formattedFacets[facetName] = aggResult.buckets.map(bucket => ({\n            key: bucket.key,\n            count: bucket.doc_count\n          }));\n        }\n      });\n    }\n    \n    return {\n      results: result.hits,\n      total: result.total,\n      facets: formattedFacets\n    };\n  }\n  \n  // Auto-complete suggestions\n  async suggest(index, text, field = 'title') {\n    try {\n      const response = await this.client.search({\n        index,\n        body: {\n          suggest: {\n            text_suggest: {\n              prefix: text,\n              completion: {\n                field: `${field}_suggest`,\n                size: 10\n              }\n            }\n          }\n        }\n      });\n      \n      const suggestions = response.body.suggest.text_suggest[0].options;\n      return suggestions.map(suggestion => ({\n        text: suggestion.text,\n        score: suggestion._score,\n        source: suggestion._source\n      }));\n    } catch (error) {\n      console.error('Suggestion error:', error);\n      throw error;\n    }\n  }\n}\n\n// Express API endpoints\nconst searchService = new SearchService();\n\n// Search endpoint\napp.get('/api/search', async (req, res) => {\n  try {\n    const { q, index = 'products', page = 1, size = 20, filters } = req.query;\n    \n    if (!q) {\n      return res.status(400).json({ error: 'Search query is required' });\n    }\n    \n    const options = {\n      from: (page - 1) * size,\n      size: parseInt(size),\n      highlight: {\n        fields: {\n          title: {},\n          description: {}\n        }\n      }\n    };\n    \n    if (filters) {\n      options.filters = JSON.parse(filters);\n    }\n    \n    const results = await searchService.multiFieldSearch(index, q, options);\n    \n    res.json({\n      query: q,\n      results: results.hits,\n      total: results.total,\n      page: parseInt(page),\n      totalPages: Math.ceil(results.total / size)\n    });\n  } catch (error) {\n    console.error('Search API error:', error);\n    res.status(500).json({ error: 'Search failed' });\n  }\n});\n\n// Faceted search endpoint\napp.get('/api/search/faceted', async (req, res) => {\n  try {\n    const { q, index = 'products' } = req.query;\n    \n    const facets = {\n      category: { type: 'terms', field: 'category.keyword' },\n      brand: { type: 'terms', field: 'brand.keyword' },\n      price_range: {\n        type: 'range',\n        field: 'price',\n        ranges: [\n          { to: 50 },\n          { from: 50, to: 100 },\n          { from: 100, to: 200 },\n          { from: 200 }\n        ]\n      }\n    };\n    \n    const results = await searchService.facetedSearch(index, q, facets);\n    \n    res.json(results);\n  } catch (error) {\n    console.error('Faceted search error:', error);\n    res.status(500).json({ error: 'Faceted search failed' });\n  }\n});\n\n// Auto-complete endpoint\napp.get('/api/search/suggest', async (req, res) => {\n  try {\n    const { q, index = 'products' } = req.query;\n    \n    if (!q || q.length < 2) {\n      return res.json({ suggestions: [] });\n    }\n    \n    const suggestions = await searchService.suggest(index, q);\n    \n    res.json({ suggestions });\n  } catch (error) {\n    console.error('Suggestion API error:', error);\n    res.status(500).json({ error: 'Suggestions failed' });\n  }\n});"
        }
      ]
    },
    {
      "id": "file-handling",
      "title": "File Handling & Processing",
      "content": [
        {
          "type": "heading",
          "text": "File Upload with Processing"
        },
        {
          "type": "paragraph",
          "text": "Handle file uploads with validation, processing, and storage optimization."
        },
        {
          "type": "code",
          "language": "javascript",
          "text": "// File upload service with image processing\nconst multer = require('multer');\nconst sharp = require('sharp');\nconst AWS = require('aws-sdk');\nconst path = require('path');\n\nclass FileUploadService {\n  constructor() {\n    this.s3 = new AWS.S3({\n      accessKeyId: process.env.AWS_ACCESS_KEY_ID,\n      secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,\n      region: process.env.AWS_REGION\n    });\n    \n    this.allowedImageTypes = ['image/jpeg', 'image/png', 'image/webp'];\n    this.allowedDocumentTypes = ['application/pdf', 'text/plain', 'application/msword'];\n    this.maxFileSize = 10 * 1024 * 1024; // 10MB\n  }\n  \n  // Configure multer for file uploads\n  getMulterConfig() {\n    const storage = multer.memoryStorage();\n    \n    return multer({\n      storage,\n      limits: {\n        fileSize: this.maxFileSize\n      },\n      fileFilter: (req, file, cb) => {\n        const allowedTypes = [...this.allowedImageTypes, ...this.allowedDocumentTypes];\n        \n        if (allowedTypes.includes(file.mimetype)) {\n          cb(null, true);\n        } else {\n          cb(new Error(`File type ${file.mimetype} not allowed`), false);\n        }\n      }\n    });\n  }\n  \n  // Process and upload image\n  async processAndUploadImage(file, options = {}) {\n    try {\n      const { width = 800, height = 600, quality = 80 } = options;\n      \n      // Process image with Sharp\n      let processedBuffer = await sharp(file.buffer)\n        .resize(width, height, {\n          fit: 'inside',\n          withoutEnlargement: true\n        })\n        .jpeg({ quality })\n        .toBuffer();\n      \n      // Generate filename\n      const filename = this.generateFilename(file.originalname, 'jpg');\n      \n      // Upload to S3\n      const uploadResult = await this.uploadToS3(\n        processedBuffer,\n        filename,\n        'image/jpeg'\n      );\n      \n      // Generate thumbnails\n      const thumbnailBuffer = await sharp(file.buffer)\n        .resize(200, 200, { fit: 'cover' })\n        .jpeg({ quality: 70 })\n        .toBuffer();\n      \n      const thumbnailFilename = this.generateFilename(file.originalname, 'thumb.jpg');\n      const thumbnailResult = await this.uploadToS3(\n        thumbnailBuffer,\n        thumbnailFilename,\n        'image/jpeg'\n      );\n      \n      return {\n        original: uploadResult,\n        thumbnail: thumbnailResult,\n        metadata: {\n          originalName: file.originalname,\n          size: processedBuffer.length,\n          mimetype: 'image/jpeg',\n          dimensions: { width, height }\n        }\n      };\n    } catch (error) {\n      console.error('Image processing error:', error);\n      throw error;\n    }\n  }\n  \n  // Upload file to S3\n  async uploadToS3(buffer, filename, mimetype) {\n    const params = {\n      Bucket: process.env.S3_BUCKET_NAME,\n      Key: filename,\n      Body: buffer,\n      ContentType: mimetype,\n      CacheControl: 'max-age=31536000', // 1 year cache\n      Metadata: {\n        uploadedAt: new Date().toISOString()\n      }\n    };\n    \n    try {\n      const result = await this.s3.upload(params).promise();\n      \n      return {\n        url: result.Location,\n        key: result.Key,\n        bucket: result.Bucket,\n        etag: result.ETag\n      };\n    } catch (error) {\n      console.error('S3 upload error:', error);\n      throw error;\n    }\n  }\n  \n  // Generate unique filename\n  generateFilename(originalName, extension = null) {\n    const timestamp = Date.now();\n    const random = Math.random().toString(36).substring(2, 15);\n    const ext = extension || path.extname(originalName);\n    \n    return `uploads/${timestamp}-${random}${ext}`;\n  }\n  \n  // Delete file from S3\n  async deleteFromS3(key) {\n    try {\n      await this.s3.deleteObject({\n        Bucket: process.env.S3_BUCKET_NAME,\n        Key: key\n      }).promise();\n      \n      return true;\n    } catch (error) {\n      console.error('S3 delete error:', error);\n      throw error;\n    }\n  }\n  \n  // Get file metadata\n  async getFileMetadata(key) {\n    try {\n      const result = await this.s3.headObject({\n        Bucket: process.env.S3_BUCKET_NAME,\n        Key: key\n      }).promise();\n      \n      return {\n        size: result.ContentLength,\n        contentType: result.ContentType,\n        lastModified: result.LastModified,\n        etag: result.ETag,\n        metadata: result.Metadata\n      };\n    } catch (error) {\n      console.error('Get metadata error:', error);\n      throw error;\n    }\n  }\n  \n  // Generate presigned URL for direct upload\n  generatePresignedUrl(filename, contentType, expiresIn = 3600) {\n    const params = {\n      Bucket: process.env.S3_BUCKET_NAME,\n      Key: this.generateFilename(filename),\n      ContentType: contentType,\n      Expires: expiresIn\n    };\n    \n    return this.s3.getSignedUrl('putObject', params);\n  }\n}\n\n// Express endpoints for file handling\nconst fileUploadService = new FileUploadService();\nconst upload = fileUploadService.getMulterConfig();\n\n// Single file upload\napp.post('/api/upload/image', upload.single('image'), async (req, res) => {\n  try {\n    if (!req.file) {\n      return res.status(400).json({ error: 'No file uploaded' });\n    }\n    \n    const result = await fileUploadService.processAndUploadImage(req.file, {\n      width: parseInt(req.body.width) || 800,\n      height: parseInt(req.body.height) || 600,\n      quality: parseInt(req.body.quality) || 80\n    });\n    \n    // Save file info to database\n    const fileRecord = await db.collection('files').insertOne({\n      ...result,\n      uploadedBy: req.user?.id,\n      uploadedAt: new Date()\n    });\n    \n    res.json({\n      success: true,\n      file: {\n        id: fileRecord.insertedId,\n        ...result\n      }\n    });\n  } catch (error) {\n    console.error('Upload error:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Multiple file upload\napp.post('/api/upload/multiple', upload.array('files', 10), async (req, res) => {\n  try {\n    if (!req.files || req.files.length === 0) {\n      return res.status(400).json({ error: 'No files uploaded' });\n    }\n    \n    const results = [];\n    \n    for (const file of req.files) {\n      if (fileUploadService.allowedImageTypes.includes(file.mimetype)) {\n        const result = await fileUploadService.processAndUploadImage(file);\n        results.push({ type: 'image', ...result });\n      } else {\n        // Handle non-image files\n        const filename = fileUploadService.generateFilename(file.originalname);\n        const uploadResult = await fileUploadService.uploadToS3(\n          file.buffer,\n          filename,\n          file.mimetype\n        );\n        results.push({ type: 'document', file: uploadResult });\n      }\n    }\n    \n    res.json({\n      success: true,\n      files: results\n    });\n  } catch (error) {\n    console.error('Multiple upload error:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Generate presigned URL for direct upload\napp.post('/api/upload/presigned', async (req, res) => {\n  try {\n    const { filename, contentType } = req.body;\n    \n    if (!filename || !contentType) {\n      return res.status(400).json({ error: 'Filename and content type are required' });\n    }\n    \n    const presignedUrl = fileUploadService.generatePresignedUrl(filename, contentType);\n    \n    res.json({\n      uploadUrl: presignedUrl,\n      filename: fileUploadService.generateFilename(filename)\n    });\n  } catch (error) {\n    console.error('Presigned URL error:', error);\n    res.status(500).json({ error: error.message });\n  }\n});\n\n// Delete file\napp.delete('/api/files/:id', async (req, res) => {\n  try {\n    const fileRecord = await db.collection('files').findOne({\n      _id: new ObjectId(req.params.id)\n    });\n    \n    if (!fileRecord) {\n      return res.status(404).json({ error: 'File not found' });\n    }\n    \n    // Delete from S3\n    await fileUploadService.deleteFromS3(fileRecord.original.key);\n    if (fileRecord.thumbnail) {\n      await fileUploadService.deleteFromS3(fileRecord.thumbnail.key);\n    }\n    \n    // Delete from database\n    await db.collection('files').deleteOne({ _id: new ObjectId(req.params.id) });\n    \n    res.json({ success: true });\n  } catch (error) {\n    console.error('Delete file error:', error);\n    res.status(500).json({ error: error.message });\n  }\n});"
        }
      ]
    }
  ],
  "testQuestions": [
    {
      "id": 1,
      "question": "What does ETL stand for in data processing?",
      "options": [
        "Export, Transform, Load",
        "Extract, Transform, Load",
        "Extract, Transfer, Link",
        "Execute, Test, Launch"
      ],
      "correctAnswer": 1,
      "explanation": "ETL stands for Extract, Transform, Load - the three main phases of moving and processing data from source systems to target systems."
    },
    {
      "id": 2,
      "question": "What is the main advantage of batch processing over real-time processing?",
      "options": [
        "Faster processing speed",
        "Lower resource usage and better for large volumes",
        "Real-time results",
        "Simpler implementation"
      ],
      "correctAnswer": 1,
      "explanation": "Batch processing is more efficient for large volumes of data as it can optimize resource usage and handle data in chunks, though it doesn't provide real-time results."
    },
    {
      "id": 3,
      "question": "What is the purpose of event tracking in analytics?",
      "options": [
        "To slow down the application",
        "To collect user behavior data for analysis and business intelligence",
        "To store user passwords",
        "To manage database connections"
      ],
      "correctAnswer": 1,
      "explanation": "Event tracking collects user behavior data, interactions, and business events to provide insights for analytics, business intelligence, and product optimization."
    },
    {
      "id": 4,
      "question": "What is faceted search?",
      "options": [
        "Searching multiple databases simultaneously",
        "Search with filters and categories to narrow down results",
        "Searching for faces in images",
        "A type of full-text search"
      ],
      "correctAnswer": 1,
      "explanation": "Faceted search allows users to apply multiple filters or facets (like category, price range, brand) to narrow down search results and find what they're looking for more easily."
    },
    {
      "id": 5,
      "question": "What is the purpose of image processing in file uploads?",
      "options": [
        "To make files larger",
        "To optimize size, format, and generate thumbnails",
        "To encrypt the images",
        "To add watermarks only"
      ],
      "correctAnswer": 1,
      "explanation": "Image processing optimizes uploaded images by resizing, compressing, converting formats, and generating thumbnails to improve performance and user experience."
    },
    {
      "id": 6,
      "question": "What is a presigned URL in file uploads?",
      "options": [
        "A URL that expires after one use",
        "A temporary URL that allows direct upload to cloud storage",
        "A URL for downloading files",
        "A URL that requires authentication"
      ],
      "correctAnswer": 1,
      "explanation": "A presigned URL is a temporary, time-limited URL that allows clients to upload files directly to cloud storage (like S3) without exposing credentials or going through the server."
    },
    {
      "id": 7,
      "question": "What is the difference between synchronous and asynchronous data processing?",
      "options": [
        "Synchronous is faster",
        "Synchronous blocks execution, asynchronous doesn't",
        "Asynchronous uses more memory",
        "They are the same thing"
      ],
      "correctAnswer": 1,
      "explanation": "Synchronous processing blocks execution until completion, while asynchronous processing allows other operations to continue while data is being processed in the background."
    },
    {
      "id": 8,
      "question": "What is the purpose of data aggregation in analytics?",
      "options": [
        "To slow down queries",
        "To combine and summarize data for reporting and insights",
        "To encrypt data",
        "To backup data"
      ],
      "correctAnswer": 1,
      "explanation": "Data aggregation combines and summarizes raw data into meaningful metrics and insights, such as totals, averages, and trends for reporting and analysis."
    },
    {
      "id": 9,
      "question": "What is stream processing?",
      "options": [
        "Processing data in batches",
        "Processing data in real-time as it flows through the system",
        "Processing only video streams",
        "Processing data once per day"
      ],
      "correctAnswer": 1,
      "explanation": "Stream processing handles data in real-time as it flows through the system, allowing for immediate analysis and response to events as they occur."
    },
    {
      "id": 10,
      "question": "What is the main benefit of using CDN (Content Delivery Network) for file storage?",
      "options": [
        "Cheaper storage costs",
        "Faster file delivery through geographically distributed servers",
        "Better security",
        "Automatic file compression"
      ],
      "correctAnswer": 1,
      "explanation": "CDN improves file delivery performance by serving files from geographically distributed servers closer to users, reducing latency and improving user experience."
    }
  ]
} 